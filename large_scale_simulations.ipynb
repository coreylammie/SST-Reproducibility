{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKRhsS2eEGFE"
   },
   "source": [
    "## Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC-_3l07EGFF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "L1BqTjOGEGFI",
    "outputId": "33420c18-3d39-4928-b411-6a6a82c0cfc1"
   },
   "outputs": [],
   "source": [
    "!pip install memtorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lz8ZHwREGFL"
   },
   "source": [
    "## Define and train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "00568dbac19c4efd94a871745b324e60",
      "2187882923284f0db960f307011068fd",
      "99e0efdbe8d84eda87663408eeb22496",
      "66c5f7efb9e54c128c2ecb48e12563ea",
      "230147f825de483aa3f611a7893ef774",
      "8b215e78206c43d4886601bb38cfd815",
      "9859d0b889c84089831d2ac5320bd6f9",
      "ee4b1ae13df84e7ab7d7a7f2fee68ffc"
     ]
    },
    "id": "1aSs8xMiEGFL",
    "outputId": "26a78a81-f53f-4ad9-b622-0a26dc556617"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import memtorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, inflation_ratio=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=128*inflation_ratio, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn0 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act0 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=128*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=512*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512*inflation_ratio)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(in_channels=512*inflation_ratio, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(in_features=512*4*4, out_features=1024)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act7 = nn.ReLU()\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.act0(self.bn0(self.conv0(input)))\n",
    "        x = self.act1(self.bn1(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = self.act2(self.bn2(self.conv2(x)))\n",
    "        x = self.act3(self.bn3(F.max_pool2d(self.conv3(x), 2)))\n",
    "        x = self.act4(self.bn4(self.conv4(x)))\n",
    "        x = self.act5(self.bn5(F.max_pool2d(self.conv5(x), 2)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act6(self.bn6(self.fc6(x)))\n",
    "        x = self.act7(self.bn7(self.fc7(x)))\n",
    "        return self.fc8(x)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):        \n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cpu' if 'cpu' in memtorch.__version__ else 'cuda')\n",
    "epochs = 60\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=512, validation=False)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_accuracy = 0\n",
    "for epoch in range(0, epochs):\n",
    "    print('Epoch: [%d]\\t\\t' % (epoch + 1), end='')\n",
    "    if epoch % 20 == 0:\n",
    "        learning_rate = learning_rate * 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = test(model, test_loader)\n",
    "    print('%2.2f%%' % accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), 'trained_model.pt')\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1KCZPp9EGFN"
   },
   "source": [
    "## Validate the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "UxYRHvMiEGFO",
    "outputId": "b92dcec1-477f-4e3f-808d-3df10b981d3e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import memtorch\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cpu' if 'cpu' in memtorch.__version__ else 'cuda')\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=264, validation=False)\n",
    "model = Net().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "except:\n",
    "    raise Exception('trained_model.pt has not been found.')\n",
    "\n",
    "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Gws_GLPEGFT"
   },
   "source": [
    "## Device endurance (gradual) simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z3GixVlEGFT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from enum import Enum, auto\n",
    "from memtorch.mn.Module import supported_module_parameters\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Module import naive_tune\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def minimal_tune(model):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if hasattr(m, 'tune'):\n",
    "            m.transform_output = lambda input: input\n",
    "            if isinstance(m, memtorch.mn.Conv2d):\n",
    "                try:\n",
    "                    m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
    "                except:\n",
    "                    pass\n",
    "            if isinstance(m, memtorch.mn.Linear):\n",
    "                try:\n",
    "                    m.transform_output = naive_tune(m, (64, m.in_features))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return model\n",
    "    \n",
    "def update_patched_model(patched_model, model):\n",
    "    for i, (name, m) in enumerate(list(patched_model.named_modules())):\n",
    "        if isinstance(m, memtorch.mn.Conv2d) or isinstance(m, memtorch.mn.Linear):\n",
    "            pos_conductance_matrix, neg_conductance_matrix = naive_map(getattr(model, name).weight.data, r_on, r_off,scheme=memtorch.bh.Scheme.DoubleColumn)\n",
    "            m.crossbars[0].write_conductance_matrix(pos_conductance_matrix, transistor=True, programming_routine=None)\n",
    "            m.crossbars[1].write_conductance_matrix(neg_conductance_matrix, transistor=True, programming_routine=None)\n",
    "            m.weight.data = getattr(model, name).weight.data\n",
    "            \n",
    "    return patched_model\n",
    "    \n",
    "scale_input = interp1d([1.3, 1.9], [0, 1])\n",
    "def scale_p_0(p_0, p_1, v_stop, cell_size=10):\n",
    "    scaled_input = scale_input(v_stop)\n",
    "    x = 1.50\n",
    "    y = p_0 * math.exp(p_1 * cell_size)\n",
    "    k = math.log10(y) / (1 - (2 * scale_input(x) - 1) ** (2))\n",
    "    new_y = 10 ** (k * (1 - (2 * scaled_input - 1) ** (2)))\n",
    "    # Backsolve for p_0\n",
    "    p_0 = new_y / math.exp(p_1 * cell_size)\n",
    "    return p_0\n",
    "\n",
    "def gradual(input, cycle_count, p_1, p_2, p_3, cell_size):\n",
    "    p_0 = torch.log10(input)\n",
    "    threshold = p_1 * math.exp(p_2 * cell_size)\n",
    "    return torch.pow(10, (p_3 * cell_size * math.log10(cycle_count) + torch.log10(10 **  p_0) - p_3 * cell_size * math.log10(threshold)))\n",
    "       \n",
    "def model_gradual(layer, cycle_count, v_stop):\n",
    "    cell_size = 10\n",
    "    convergence_point = 1e4\n",
    "    p_1_lrs = 1.0399076623425807\n",
    "    p_2_lrs = 0.9171208448973687\n",
    "    p_3_lrs = 0.0143551595777695\n",
    "    p_1_hrs = 4.3590883730463410\n",
    "    p_2_hrs = 0.7738077425228179\n",
    "    p_3_hrs = -0.018865423084966\n",
    "    p_1_lrs = scale_p_0(p_1_lrs, p_2_lrs, v_stop)\n",
    "    p_1_hrs = scale_p_0(p_1_hrs, p_2_hrs, v_stop)\n",
    "    threshold_lrs = p_1_lrs * math.exp(p_2_lrs * cell_size)\n",
    "    threshold_hrs = p_1_hrs * math.exp(p_2_hrs * cell_size)\n",
    "    for i in range(len(layer.crossbars)):\n",
    "        input = 1 / layer.crossbars[i].conductance_matrix\n",
    "        if input[input < convergence_point].nelement() > 0:\n",
    "            if cycle_count > threshold_lrs:\n",
    "                input[input < convergence_point] = gradual(input[input < convergence_point], cycle_count, p_1_lrs, p_2_lrs, p_3_lrs, cell_size)\n",
    "        if input[input > convergence_point].nelement() > 0:\n",
    "            if cycle_count > threshold_hrs:\n",
    "                input[input > convergence_point] = gradual(input[input > convergence_point], cycle_count, p_1_hrs, p_2_hrs, p_3_hrs, cell_size)\n",
    "                \n",
    "        layer.crossbars[i].conductance_matrix = 1 / input\n",
    "\n",
    "    return layer\n",
    "\n",
    "def model_degradation(model, cycle_count, v_stop):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if type(m) in supported_module_parameters.values():\n",
    "            setattr(model, name, model_gradual(m, cycle_count, v_stop)) # setattr(model.module, name, model_gradual(m, cycle_count, v_stop))\n",
    "\n",
    "                    \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda')\n",
    "batch_size = 64\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=batch_size, validation=False)\n",
    "reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "r_on = 4400\n",
    "r_off = 65000\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
    "                              'r_off': r_off,\n",
    "                              'r_on': r_on}\n",
    "times_to_reprogram = 10 ** np.arange(1, 10, dtype=np.float64)\n",
    "v_stop_values = np.linspace(1.3, 1.9, 10, endpoint=True)\n",
    "df = pd.DataFrame(columns=['times_reprogramed', 'v_stop', 'test_set_accuracy'])\n",
    "for time_to_reprogram in times_to_reprogram:\n",
    "    cycle_count = len(train_loader.dataset) * time_to_reprogram\n",
    "    for v_stop in v_stop_values:\n",
    "        print('time_to_reprogram: %f, v_stop: %f' % (time_to_reprogram, v_stop))\n",
    "        model = Net().to(device)\n",
    "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "        patched_model = patch_model(model,\n",
    "                                  memristor_model=reference_memristor,\n",
    "                                  memristor_model_params=reference_memristor_params,\n",
    "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                                  mapping_routine=naive_map,\n",
    "                                  transistor=True,\n",
    "                                  programming_routine=None,\n",
    "                                  p_l=None,\n",
    "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
    "        patched_model = model_degradation(patched_model, cycle_count, v_stop)\n",
    "        patched_model = minimal_tune(patched_model)\n",
    "        accuracy = test(patched_model, test_loader)\n",
    "        del patched_model\n",
    "        del model\n",
    "        df = df.append({'times_reprogramed': time_to_reprogram, 'v_stop': v_stop, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
    "        df.to_csv('endurance_gradual.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAoJaCkkEGFX"
   },
   "source": [
    "## Device endurance (sudden) simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wk7iLeSQEGFX",
    "outputId": "97fd4965-88a9-4ae1-e11f-bbe3f756c043"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from enum import Enum, auto\n",
    "from memtorch.mn.Module import supported_module_parameters\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Module import naive_tune\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
    "import copy\n",
    "\n",
    "def minimal_tune(model):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if hasattr(m, 'tune'):\n",
    "            m.transform_output = lambda input: input\n",
    "            if isinstance(m, memtorch.mn.Conv2d):\n",
    "                try:\n",
    "                    m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
    "                except:\n",
    "                    pass\n",
    "            if isinstance(m, memtorch.mn.Linear):\n",
    "                try:\n",
    "                    m.transform_output = naive_tune(m, (64, m.in_features))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return model\n",
    "    \n",
    "def update_patched_model(patched_model, model):\n",
    "    for i, (name, m) in enumerate(list(patched_model.named_modules())):\n",
    "        if isinstance(m, memtorch.mn.Conv2d) or isinstance(m, memtorch.mn.Linear):\n",
    "            pos_conductance_matrix, neg_conductance_matrix = naive_map(getattr(model, name).weight.data, r_on, r_off,scheme=memtorch.bh.Scheme.DoubleColumn)\n",
    "            m.crossbars[0].write_conductance_matrix(pos_conductance_matrix, transistor=True, programming_routine=None)\n",
    "            m.crossbars[1].write_conductance_matrix(neg_conductance_matrix, transistor=True, programming_routine=None)\n",
    "            m.weight.data = getattr(model, name).weight.data\n",
    "            \n",
    "    return patched_model\n",
    "\n",
    "scale_input = interp1d([1.3, 1.9], [0, 1])\n",
    "def scale_p_0(p_0, p_1, v_stop, cell_size=10):\n",
    "    scaled_input = scale_input(v_stop)\n",
    "    x = 1.45\n",
    "    y = p_0 * cell_size + p_1\n",
    "    k = math.log10(y) / (1 - (2 * scale_input(x) - 1) ** (2))\n",
    "    new_y = 10 ** (k * (1 - (2 * scaled_input - 1) ** (2)))\n",
    "    p_0 = (new_y - p_1) / cell_size\n",
    "    return p_0\n",
    "       \n",
    "def model_sudden(layer, cycle_count, v_stop):\n",
    "    cell_size = 10\n",
    "    p_1 = 0\n",
    "    p_0 = 2e7 / cell_size\n",
    "    p_0 = scale_p_0(p_0, p_1, v_stop)\n",
    "    threshold = p_0 * cell_size\n",
    "    if cycle_count > threshold:\n",
    "        for i in range(len(layer.crossbars)):\n",
    "            input = layer.crossbars[i].conductance_matrix\n",
    "            input[input < (1 / 2e4)] = 1 / 2e4\n",
    "            layer.crossbars[i].conductance_matrix = input\n",
    "            \n",
    "    return layer\n",
    "\n",
    "def model_degradation(model, cycle_count, v_stop):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if type(m) in supported_module_parameters.values():\n",
    "            setattr(model, name, model_sudden(m, cycle_count, v_stop))\n",
    "             \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda')\n",
    "batch_size = 64\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=batch_size, validation=False)\n",
    "reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "r_on = 2.00e4\n",
    "r_off = 10.75e4\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
    "                              'r_off': r_off,\n",
    "                              'r_on': r_on}\n",
    "times_to_reprogram = 10 ** np.arange(1, 10, dtype=np.float64)\n",
    "v_stop_values = np.linspace(1.3, 1.9, 10, endpoint=True)\n",
    "df = pd.DataFrame(columns=['times_reprogramed', 'v_stop', 'test_set_accuracy'])\n",
    "for time_to_reprogram in times_to_reprogram:\n",
    "    cycle_count = len(train_loader.dataset) * time_to_reprogram\n",
    "    for v_stop in v_stop_values:\n",
    "        print('time_to_reprogram: %f, v_stop: %f' % (time_to_reprogram, v_stop))\n",
    "        model = Net().to(device)\n",
    "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "        patched_model = patch_model(model,\n",
    "                                  memristor_model=reference_memristor,\n",
    "                                  memristor_model_params=reference_memristor_params,\n",
    "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                                  mapping_routine=naive_map,\n",
    "                                  transistor=True,\n",
    "                                  programming_routine=None,\n",
    "                                  p_l=None,\n",
    "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
    "        patched_model = model_degradation(patched_model, cycle_count, v_stop)\n",
    "        patched_model = minimal_tune(patched_model)\n",
    "        accuracy = test(patched_model, test_loader)\n",
    "        del patched_model\n",
    "        del model\n",
    "        df = df.append({'times_reprogramed': time_to_reprogram, 'v_stop': v_stop, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
    "        df.to_csv('endurance_sudden.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcRNelpJEGFZ"
   },
   "source": [
    "## Device retention (gradual) simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwHbxLqmEGFa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "from enum import Enum, auto\n",
    "import math\n",
    "import pandas as pd\n",
    "from memtorch.mn.Module import supported_module_parameters\n",
    "import math\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Module import naive_tune\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
    "import copy\n",
    "\n",
    "\n",
    "class OperationMode(Enum):\n",
    "    sudden = auto()\n",
    "    gradual = auto()\n",
    "\n",
    "    \n",
    "def minimal_tune(model):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if hasattr(m, 'tune'):\n",
    "            m.transform_output = lambda input: input\n",
    "            if isinstance(m, memtorch.mn.Conv2d):\n",
    "                m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
    "            if isinstance(m, memtorch.mn.Linear):\n",
    "                m.transform_output = naive_tune(m, (64, m.in_features))\n",
    "                \n",
    "    return model\n",
    "    \n",
    "def gradual(input, x, p_0, p_1, p_2, p_3, cell_size, tempurature):\n",
    "    p_0 = p_0 / input\n",
    "    if tempurature > 298:\n",
    "        input = torch.pow(10, p_0 * tempurature + torch.log10(input) - p_0 * 298)\n",
    "        \n",
    "    p_0 = torch.log10(input)\n",
    "    threshold = p_1 * np.exp(p_2 * cell_size)\n",
    "    return torch.pow(10, (p_3 * cell_size * math.log10(x) + torch.log10(10 **  p_0) - p_3 * cell_size * math.log10(threshold)))\n",
    "    \n",
    "def model_gradual(layer, x, tempurature):\n",
    "    cell_size = 10\n",
    "    convergence_point = 180000\n",
    "    p_0_lrs = 0.000801158151673717 * 2400\n",
    "    p_0_hrs = 0.00420717061486765 * 55000\n",
    "    p_1 = 0.5\n",
    "    p_2 = 0.7600902459542083\n",
    "    p_3_lrs = 0.006489105105825544  \n",
    "    p_3_hrs = -0.007240917429683966\n",
    "    threshold_lrs = p_1 * math.exp(p_2 * cell_size)\n",
    "    threshold_hrs = p_1 * math.exp(p_2 * cell_size)\n",
    "    for i in range(len(layer.crossbars)):\n",
    "        input = 1 / layer.crossbars[i].conductance_matrix\n",
    "        if input[input < convergence_point].nelement() > 0:\n",
    "            if x > threshold_lrs:\n",
    "                input[input < convergence_point] = gradual(input[input < convergence_point], time_, p_0_lrs, p_1, p_2, p_3_lrs, cell_size, tempurature)\n",
    "        if input[input > convergence_point].nelement() > 0:\n",
    "            if x > threshold_hrs:\n",
    "                input[input > convergence_point] = gradual(input[input > convergence_point], time_, p_0_hrs, p_1, p_2, p_3_hrs, cell_size, tempurature)\n",
    "\n",
    "        layer.crossbars[i].conductance_matrix = 1 / input\n",
    "\n",
    "    return layer\n",
    "    \n",
    "def model_degradation(model, time_, operation_mode, tempurature):\n",
    "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
    "        if type(m) in supported_module_parameters.values():\n",
    "            if len(name.split('.')) > 1:\n",
    "                name = name.split('.')[1]\n",
    "\n",
    "            if operation_mode == OperationMode.gradual:\n",
    "                if hasattr(model, 'module'):\n",
    "                    setattr(model.module, name, model_gradual(m, time_, tempurature))\n",
    "                else:\n",
    "                    setattr(model, name, model_gradual(m, time_, tempurature))\n",
    "            elif operation_mode == OperationMode.sudden:\n",
    "                if hasattr(model, 'module'):\n",
    "                    setattr(model.module, name, model_sudden(m, time_, tempurature))\n",
    "                else:\n",
    "                    setattr(model, name, model_sudden(m, time_, tempurature))\n",
    "                    \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda')\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=64, validation=False)\n",
    "reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "r_on = 9e4\n",
    "r_off = 330000\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
    "                              'r_off': r_off,\n",
    "                              'r_on': r_on}\n",
    "times = 10 ** np.arange(1, 10, dtype=np.float64)\n",
    "tempuratures = np.linspace(75, 175, 10, endpoint=True)\n",
    "df = pd.DataFrame(columns=['time', 'tempurature', 'test_set_accuracy'])\n",
    "for time_ in times:\n",
    "    print(time_)\n",
    "    for tempurature in tempuratures:\n",
    "        tempurature += 273\n",
    "        model = Net().to(device)\n",
    "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "        patched_model = patch_model(model,\n",
    "                                  memristor_model=reference_memristor,\n",
    "                                  memristor_model_params=reference_memristor_params,\n",
    "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                                  mapping_routine=naive_map,\n",
    "                                  transistor=True,\n",
    "                                  programming_routine=None,\n",
    "                                  p_l=None,\n",
    "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
    "\n",
    "        patched_model = minimal_tune(patched_model)\n",
    "        patched_model = model_degradation(patched_model, time_, OperationMode.gradual, tempurature)\n",
    "        accuracy = test(patched_model, test_loader)\n",
    "        del patched_model\n",
    "        df = df.append({'time': time_, 'tempurature': tempurature, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
    "        df.to_csv('retention_gradual.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Final.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00568dbac19c4efd94a871745b324e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99e0efdbe8d84eda87663408eeb22496",
       "IPY_MODEL_66c5f7efb9e54c128c2ecb48e12563ea"
      ],
      "layout": "IPY_MODEL_2187882923284f0db960f307011068fd"
     }
    },
    "2187882923284f0db960f307011068fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "230147f825de483aa3f611a7893ef774": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "66c5f7efb9e54c128c2ecb48e12563ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee4b1ae13df84e7ab7d7a7f2fee68ffc",
      "placeholder": "​",
      "style": "IPY_MODEL_9859d0b889c84089831d2ac5320bd6f9",
      "value": " 170500096/? [00:20&lt;00:00, 31869231.39it/s]"
     }
    },
    "8b215e78206c43d4886601bb38cfd815": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9859d0b889c84089831d2ac5320bd6f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99e0efdbe8d84eda87663408eeb22496": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b215e78206c43d4886601bb38cfd815",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_230147f825de483aa3f611a7893ef774",
      "value": 1
     }
    },
    "ee4b1ae13df84e7ab7d7a7f2fee68ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
