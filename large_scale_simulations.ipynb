{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "large_scale_simulations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m48",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKRhsS2eEGFE"
      },
      "source": [
        "## Required imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p00wfQBwWDQu"
      },
      "source": [
        "SAVE_GOOGLE_COLAB = True # Flag used to save results to a Google Drive when operating on Google Colab\n",
        "import shutil\n",
        "if SAVE_GOOGLE_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-_3l07EGFF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1BqTjOGEGFI",
        "outputId": "e48a59ac-84dc-43b0-9d7c-af8eae01efd3"
      },
      "source": [
        "!pip install memtorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting memtorch\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/12/e2432f1ae2bed6020bf83b4ceb2ac4e7248e273a32de0e776e85d13de087/memtorch-1.0.9.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from memtorch) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from memtorch) (1.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from memtorch) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from memtorch) (0.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from memtorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from memtorch) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from memtorch) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->memtorch) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->memtorch) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->memtorch) (0.22.2.post1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->memtorch) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->memtorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->memtorch) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->memtorch) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->memtorch) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->memtorch) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->memtorch) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->memtorch) (0.17.0)\n",
            "Building wheels for collected packages: memtorch\n",
            "  Building wheel for memtorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memtorch: filename=memtorch-1.0.9-cp36-cp36m-linux_x86_64.whl size=3601015 sha256=5008e09fde921ef2bb283be8059a3a3f28b7ee6b331d1c890158c49710b6f020\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/69/77/b6119c8e13d6607a62e0af8fb870868375cb892be09875b4bb\n",
            "Successfully built memtorch\n",
            "Installing collected packages: memtorch\n",
            "Successfully installed memtorch-1.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lz8ZHwREGFL"
      },
      "source": [
        "## Define and train the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aSs8xMiEGFL",
        "outputId": "916e9601-a2a8-45ca-f324-9dbf39faa78a"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import memtorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision\n",
        "from mobilenetv2 import MobileNetV2\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):        \n",
        "        output = model(data.to(device))\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
        "\n",
        "set_all_seeds(0)\n",
        "device = torch.device('cpu' if 'cpu' in memtorch.__version__ else 'cuda')\n",
        "epochs = 100\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=1)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
        "model = MobileNetV2().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
        "best_accuracy = 0\n",
        "for epoch in range(0, epochs):\n",
        "    print('Epoch: [%d]\\t\\t' % (epoch + 1), end='')\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.to(device))\n",
        "        loss = criterion(output, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    accuracy = test(model, test_loader)\n",
        "    print('%2.2f%%' % accuracy)\n",
        "    if accuracy > best_accuracy:\n",
        "        print('Saving model...')\n",
        "        torch.save(model.state_dict(), 'trained_model.pt')\n",
        "        best_accuracy = accuracy\n",
        "        \n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: [1]\t\t47.07%\n",
            "Saving model...\n",
            "Epoch: [2]\t\t61.04%\n",
            "Saving model...\n",
            "Epoch: [3]\t\t68.69%\n",
            "Saving model...\n",
            "Epoch: [4]\t\t73.00%\n",
            "Saving model...\n",
            "Epoch: [5]\t\t75.79%\n",
            "Saving model...\n",
            "Epoch: [6]\t\t78.96%\n",
            "Saving model...\n",
            "Epoch: [7]\t\t79.38%\n",
            "Saving model...\n",
            "Epoch: [8]\t\t81.83%\n",
            "Saving model...\n",
            "Epoch: [9]\t\t82.41%\n",
            "Saving model...\n",
            "Epoch: [10]\t\t80.94%\n",
            "Epoch: [11]\t\t84.21%\n",
            "Saving model...\n",
            "Epoch: [12]\t\t83.76%\n",
            "Epoch: [13]\t\t83.79%\n",
            "Epoch: [14]\t\t85.84%\n",
            "Saving model...\n",
            "Epoch: [15]\t\t85.42%\n",
            "Epoch: [16]\t\t85.63%\n",
            "Epoch: [17]\t\t86.46%\n",
            "Saving model...\n",
            "Epoch: [18]\t\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KCZPp9EGFN"
      },
      "source": [
        "## Validate the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxYRHvMiEGFO",
        "outputId": "4aec562c-ddde-417e-b58d-91341e2795b0"
      },
      "source": [
        "import torch\n",
        "import memtorch\n",
        "from memtorch.utils import LoadCIFAR10\n",
        "import numpy as np\n",
        "from mobilenetv2 import MobileNetV2\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        output = model(data.to(device))\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
        "model = MobileNetV2().to(device)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
        "except:\n",
        "    raise Exception('trained_model.pt has not been found.')\n",
        "\n",
        "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test Set Accuracy: \t87.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gws_GLPEGFT"
      },
      "source": [
        "## Device endurance (gradual) simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z3GixVlEGFT",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea3b3eaa-695b-4f75-e6d9-cfbf3796cc8b"
      },
      "source": [
        "import enum\n",
        "from enum import Enum, auto\n",
        "from memtorch.mn.Module import supported_module_parameters\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from memtorch.mn.Module import patch_model\n",
        "from memtorch.map.Module import naive_tune\n",
        "from memtorch.map.Parameter import naive_map\n",
        "from memtorch.bh.crossbar.Program import naive_program\n",
        "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
        "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
        "import copy\n",
        "from pprint import pprint\n",
        "from mobilenetv2 import MobileNetV2\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "def minimal_tune(model):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if hasattr(m, 'tune'):\n",
        "            m.transform_output = lambda input: input\n",
        "            if isinstance(m, memtorch.mn.Conv2d):\n",
        "                try:\n",
        "                    m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
        "                except:\n",
        "                    pass\n",
        "            if isinstance(m, memtorch.mn.Linear):\n",
        "                try:\n",
        "                    m.transform_output = naive_tune(m, (64, m.in_features))\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "    return model.to(device)\n",
        "    \n",
        "def update_patched_model(patched_model, model):\n",
        "    for i, (name, m) in enumerate(list(patched_model.named_modules())):\n",
        "        if isinstance(m, memtorch.mn.Conv2d) or isinstance(m, memtorch.mn.Linear):\n",
        "            pos_conductance_matrix, neg_conductance_matrix = naive_map(getattr(model, name).weight.data, r_on, r_off,scheme=memtorch.bh.Scheme.DoubleColumn)\n",
        "            m.crossbars[0].write_conductance_matrix(pos_conductance_matrix, transistor=True, programming_routine=None)\n",
        "            m.crossbars[1].write_conductance_matrix(neg_conductance_matrix, transistor=True, programming_routine=None)\n",
        "            m.weight.data = getattr(model, name).weight.data\n",
        "            \n",
        "    return patched_model\n",
        "    \n",
        "scale_input = interp1d([1.3, 1.9], [0, 1])\n",
        "def scale_p_0(p_0, p_1, v_stop, cell_size=10):\n",
        "    scaled_input = scale_input(v_stop)\n",
        "    x = 1.50\n",
        "    y = p_0 * math.exp(p_1 * cell_size)\n",
        "    k = math.log10(y) / (1 - (2 * scale_input(x) - 1) ** (2))\n",
        "    new_y = 10 ** (k * (1 - (2 * scaled_input - 1) ** (2)))\n",
        "    # Backsolve for p_0\n",
        "    p_0 = new_y / math.exp(p_1 * cell_size)\n",
        "    return p_0\n",
        "\n",
        "def gradual(input, cycle_count, p_1, p_2, p_3, cell_size):\n",
        "    p_0 = torch.log10(input)\n",
        "    threshold = p_1 * math.exp(p_2 * cell_size)\n",
        "    return torch.pow(10, (p_3 * cell_size * math.log10(cycle_count) + torch.log10(10 **  p_0) - p_3 * cell_size * math.log10(threshold)))\n",
        "       \n",
        "def model_gradual(layer, cycle_count, v_stop):\n",
        "    cell_size = 10\n",
        "    convergence_point = 1e4\n",
        "    p_1_lrs = 1.0399076623425807\n",
        "    p_2_lrs = 0.9171208448973687\n",
        "    p_3_lrs = 0.0143551595777695\n",
        "    p_1_hrs = 4.3590883730463410\n",
        "    p_2_hrs = 0.7738077425228179\n",
        "    p_3_hrs = -0.018865423084966\n",
        "    p_1_lrs = scale_p_0(p_1_lrs, p_2_lrs, v_stop)\n",
        "    p_1_hrs = scale_p_0(p_1_hrs, p_2_hrs, v_stop)\n",
        "    threshold_lrs = p_1_lrs * math.exp(p_2_lrs * cell_size)\n",
        "    threshold_hrs = p_1_hrs * math.exp(p_2_hrs * cell_size)\n",
        "    for i in range(len(layer.crossbars)):\n",
        "        input = 1 / layer.crossbars[i].conductance_matrix\n",
        "        if input[input < convergence_point].nelement() > 0:\n",
        "            if cycle_count > threshold_lrs:\n",
        "                input[input < convergence_point] = gradual(input[input < convergence_point], cycle_count, p_1_lrs, p_2_lrs, p_3_lrs, cell_size)\n",
        "        if input[input > convergence_point].nelement() > 0:\n",
        "            if cycle_count > threshold_hrs:\n",
        "                input[input > convergence_point] = gradual(input[input > convergence_point], cycle_count, p_1_hrs, p_2_hrs, p_3_hrs, cell_size)\n",
        "                \n",
        "        layer.crossbars[i].conductance_matrix = 1 / input\n",
        "\n",
        "    return layer\n",
        "\n",
        "def model_degradation(model, cycle_count, v_stop):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if type(m) in supported_module_parameters.values():\n",
        "            setattr(model, name, model_gradual(m, cycle_count, v_stop)) # setattr(model.module, name, model_gradual(m, cycle_count, v_stop))\n",
        "\n",
        "                    \n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda')\n",
        "batch_size = 64\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=1)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
        "reference_memristor = memtorch.bh.memristor.VTEAM\n",
        "r_on = 4400\n",
        "r_off = 65000\n",
        "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
        "                              'r_off': r_off,\n",
        "                              'r_on': r_on}\n",
        "times_to_reprogram = 10 ** np.arange(1, 10, dtype=np.float64)\n",
        "v_stop_values = np.linspace(1.3, 1.9, 10, endpoint=True)\n",
        "df = pd.DataFrame(columns=['times_reprogramed', 'v_stop', 'test_set_accuracy'])\n",
        "for time_to_reprogram in times_to_reprogram:\n",
        "    cycle_count = len(train_loader.dataset) * time_to_reprogram\n",
        "    for v_stop in v_stop_values:\n",
        "        print('time_to_reprogram: %f, v_stop: %f' % (time_to_reprogram, v_stop))\n",
        "        model = MobileNetV2().to(device)\n",
        "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
        "        patched_model = patch_model(model,\n",
        "                                  memristor_model=reference_memristor,\n",
        "                                  memristor_model_params=reference_memristor_params,\n",
        "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
        "                                  mapping_routine=naive_map,\n",
        "                                  transistor=True,\n",
        "                                  programming_routine=None,\n",
        "                                  p_l=None,\n",
        "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
        "        patched_model = model_degradation(patched_model, cycle_count, v_stop)\n",
        "        patched_model = minimal_tune(patched_model)\n",
        "        accuracy = test(patched_model, test_loader)\n",
        "        del patched_model\n",
        "        del model\n",
        "        df = df.append({'times_reprogramed': time_to_reprogram, 'v_stop': v_stop, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
        "        df.to_csv('endurance_gradual.csv', index=False)\n",
        "        if SAVE_GOOGLE_COLAB:\n",
        "          shutil.copy(\"endurance_gradual.csv\", \"/content/gdrive/My Drive/endurance_gradual.csv\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "time_to_reprogram: 10.000000, v_stop: 1.300000\n",
            "Patched Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) -> bh.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False) -> bh.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=16, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False) -> bh.Conv2d(in_channels=96, out_channels=96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=96, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=16, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=24, out_channels=144, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False) -> bh.Conv2d(in_channels=144, out_channels=144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=144, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=24, out_channels=144, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False) -> bh.Conv2d(in_channels=144, out_channels=144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "Patched Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=144, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "Patched Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=96, out_channels=576, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False) -> bh.Conv2d(in_channels=576, out_channels=576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=576, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=96, out_channels=576, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False) -> bh.Conv2d(in_channels=576, out_channels=576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=576, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=96, out_channels=576, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False) -> bh.Conv2d(in_channels=576, out_channels=576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "Patched Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=576, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=160, out_channels=960, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False) -> bh.Conv2d(in_channels=960, out_channels=960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=960, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=160, out_channels=960, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False) -> bh.Conv2d(in_channels=960, out_channels=960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=960, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=160, out_channels=960, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False) -> bh.Conv2d(in_channels=960, out_channels=960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=960, out_channels=320, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=160, out_channels=320, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=320, out_channels=1280, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Linear(in_features=1280, out_features=10, bias=True) -> bh.Linear(in_features=1280, out_features=10, bias=True)\n",
            "Tuned bh.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)). Coefficient of determination: 0.615475 [983.881775, 0.014802]\n",
            "Tuned bh.Conv2d(in_channels=320, out_channels=1280, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.719407 [58.085247, -0.004266]\n",
            "Tuned bh.Linear(in_features=1280, out_features=10, bias=True). Coefficient of determination: 0.533525 [148.051819, -0.056139]\n",
            "Tuned bh.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.493250 [511.962280, 0.007892]\n",
            "Tuned bh.Conv2d(in_channels=16, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.322783 [382.169769, -0.000784]\n",
            "Tuned bh.Conv2d(in_channels=144, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.675536 [293.081512, -0.002866]\n",
            "Tuned bh.Conv2d(in_channels=144, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.865253 [467.423920, 0.009848]\n",
            "Tuned bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.698725 [272.586639, 0.003249]\n",
            "Tuned bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.552338 [217.733475, -0.002066]\n",
            "Tuned bh.Conv2d(in_channels=192, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.781031 [323.451630, -0.002455]\n",
            "Tuned bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.677702 [195.161133, -0.007414]\n",
            "Tuned bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.616360 [167.154861, -0.001580]\n",
            "Tuned bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.691581 [164.560501, 0.001034]\n",
            "Tuned bh.Conv2d(in_channels=64, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.426490 [190.318375, 0.007909]\n",
            "Tuned bh.Conv2d(in_channels=576, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.695286 [117.407677, 0.002976]\n",
            "Tuned bh.Conv2d(in_channels=576, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.751305 [116.260010, -0.000569]\n",
            "Tuned bh.Conv2d(in_channels=576, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.867149 [151.794540, 0.001731]\n",
            "Tuned bh.Conv2d(in_channels=960, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.817311 [85.989182, -0.000746]\n",
            "Tuned bh.Conv2d(in_channels=960, out_channels=160, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.920360 [87.510208, 0.000793]\n",
            "Tuned bh.Conv2d(in_channels=160, out_channels=320, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.623687 [84.565544, -0.000095]\n",
            "time_to_reprogram: 10.000000, v_stop: 1.366667\n",
            "Patched Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) -> bh.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False) -> bh.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=16, out_channels=96, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False) -> bh.Conv2d(in_channels=96, out_channels=96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=96, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=16, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=24, out_channels=144, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False) -> bh.Conv2d(in_channels=144, out_channels=144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=144, out_channels=24, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=24, out_channels=144, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False) -> bh.Conv2d(in_channels=144, out_channels=144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "Patched Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=144, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=32, out_channels=192, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False) -> bh.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "Patched Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=192, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=64, out_channels=384, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False) -> bh.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Patched Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) -> bh.Conv2d(in_channels=384, out_channels=64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c94aa00f2db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m                                   \u001b[0mprogramming_routine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                                   \u001b[0mp_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                   scheme=memtorch.bh.Scheme.DoubleColumn)\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mpatched_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_degradation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatched_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mpatched_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimal_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatched_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/memtorch/mn/Module.py\u001b[0m in \u001b[0;36mpatch_model\u001b[0;34m(model, memristor_model, memristor_model_params, module_parameters_to_patch, mapping_routine, p_l, transistor, programming_routine, programming_routine_params, scheme, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                                       \u001b[0mp_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                                       \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                                       **kwargs))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtune_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/memtorch/mn/Conv2d.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, convolutional_layer, memristor_model, memristor_model_params, mapping_routine, transistor, programming_routine, programming_routine_params, p_l, scheme, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                                \u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                                                \u001b[0mp_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                                                scheme=scheme)\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Patched %s -> %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconvolutional_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/memtorch/bh/crossbar/Crossbar.py\u001b[0m in \u001b[0;36minit_crossbar\u001b[0;34m(weights, memristor_model, memristor_model_params, transistor, mapping_routine, programming_routine, programming_routine_params, p_l, scheme)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                                                              \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                                                                              p_l=p_l)\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mcrossbars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_conductance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_conductance_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransistor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransistor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogramming_routine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mcrossbars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_conductance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_conductance_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransistor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransistor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogramming_routine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/memtorch/bh/crossbar/Crossbar.py\u001b[0m in \u001b[0;36mwrite_conductance_matrix\u001b[0;34m(self, conductance_matrix, transistor, programming_routine, programming_routine_params)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransistor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconductance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconductance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mprogramming_routine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'programming_routine must be defined if transistor is False.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/memtorch/bh/crossbar/Crossbar.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, from_devices, parallelize)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_conductance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconductance_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_conductance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconductance_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransistor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogramming_routine_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAoJaCkkEGFX"
      },
      "source": [
        "## Device endurance (sudden) simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7iLeSQEGFX"
      },
      "source": [
        "import enum\n",
        "from enum import Enum, auto\n",
        "from memtorch.mn.Module import supported_module_parameters\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from memtorch.mn.Module import patch_model\n",
        "from memtorch.map.Module import naive_tune\n",
        "from memtorch.map.Parameter import naive_map\n",
        "from memtorch.bh.crossbar.Program import naive_program\n",
        "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
        "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
        "import copy\n",
        "from mobilenetv2 import MobileNetV2\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "def minimal_tune(model):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if hasattr(m, 'tune'):\n",
        "            m.transform_output = lambda input: input\n",
        "            if isinstance(m, memtorch.mn.Conv2d):\n",
        "                try:\n",
        "                    m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
        "                except:\n",
        "                    pass\n",
        "            if isinstance(m, memtorch.mn.Linear):\n",
        "                try:\n",
        "                    m.transform_output = naive_tune(m, (64, m.in_features))\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "    return model.to(device)\n",
        "    \n",
        "def update_patched_model(patched_model, model):\n",
        "    for i, (name, m) in enumerate(list(patched_model.named_modules())):\n",
        "        if isinstance(m, memtorch.mn.Conv2d) or isinstance(m, memtorch.mn.Linear):\n",
        "            pos_conductance_matrix, neg_conductance_matrix = naive_map(getattr(model, name).weight.data, r_on, r_off,scheme=memtorch.bh.Scheme.DoubleColumn)\n",
        "            m.crossbars[0].write_conductance_matrix(pos_conductance_matrix, transistor=True, programming_routine=None)\n",
        "            m.crossbars[1].write_conductance_matrix(neg_conductance_matrix, transistor=True, programming_routine=None)\n",
        "            m.weight.data = getattr(model, name).weight.data\n",
        "            \n",
        "    return patched_model\n",
        "\n",
        "scale_input = interp1d([1.3, 1.9], [0, 1])\n",
        "def scale_p_0(p_0, p_1, v_stop, cell_size=10):\n",
        "    scaled_input = scale_input(v_stop)\n",
        "    x = 1.45\n",
        "    y = p_0 * cell_size + p_1\n",
        "    k = math.log10(y) / (1 - (2 * scale_input(x) - 1) ** (2))\n",
        "    new_y = 10 ** (k * (1 - (2 * scaled_input - 1) ** (2)))\n",
        "    p_0 = (new_y - p_1) / cell_size\n",
        "    return p_0\n",
        "       \n",
        "def model_sudden(layer, cycle_count, v_stop):\n",
        "    cell_size = 10\n",
        "    p_1 = 0\n",
        "    p_0 = 2e7 / cell_size\n",
        "    p_0 = scale_p_0(p_0, p_1, v_stop)\n",
        "    threshold = p_0 * cell_size\n",
        "    if cycle_count > threshold:\n",
        "        for i in range(len(layer.crossbars)):\n",
        "            input = layer.crossbars[i].conductance_matrix\n",
        "            input[input < (1 / 2e4)] = 1 / 2e4\n",
        "            layer.crossbars[i].conductance_matrix = input\n",
        "            \n",
        "    return layer\n",
        "\n",
        "def model_degradation(model, cycle_count, v_stop):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if type(m) in supported_module_parameters.values():\n",
        "            setattr(model, name, model_sudden(m, cycle_count, v_stop))\n",
        "             \n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda')\n",
        "batch_size = 64\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=1)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
        "reference_memristor = memtorch.bh.memristor.VTEAM\n",
        "r_on = 2.00e4\n",
        "r_off = 10.75e4\n",
        "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
        "                              'r_off': r_off,\n",
        "                              'r_on': r_on}\n",
        "times_to_reprogram = 10 ** np.arange(1, 10, dtype=np.float64)\n",
        "v_stop_values = np.linspace(1.3, 1.9, 10, endpoint=True)\n",
        "df = pd.DataFrame(columns=['times_reprogramed', 'v_stop', 'test_set_accuracy'])\n",
        "for time_to_reprogram in times_to_reprogram:\n",
        "    cycle_count = len(train_loader.dataset) * time_to_reprogram\n",
        "    for v_stop in v_stop_values:\n",
        "        print('time_to_reprogram: %f, v_stop: %f' % (time_to_reprogram, v_stop))\n",
        "        model = MobileNetV2().to(device)\n",
        "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
        "        patched_model = patch_model(model,\n",
        "                                  memristor_model=reference_memristor,\n",
        "                                  memristor_model_params=reference_memristor_params,\n",
        "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
        "                                  mapping_routine=naive_map,\n",
        "                                  transistor=True,\n",
        "                                  programming_routine=None,\n",
        "                                  p_l=None,\n",
        "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
        "        patched_model = model_degradation(patched_model, cycle_count, v_stop)\n",
        "        patched_model = minimal_tune(patched_model)\n",
        "        accuracy = test(patched_model, test_loader)\n",
        "        del patched_model\n",
        "        del model\n",
        "        df = df.append({'times_reprogramed': time_to_reprogram, 'v_stop': v_stop, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
        "        df.to_csv('endurance_sudden.csv', index=False)\n",
        "        if SAVE_GOOGLE_COLAB:\n",
        "          shutil.copy(\"endurance_sudden.csv\", \"/content/gdrive/My Drive/endurance_sudden.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcRNelpJEGFZ"
      },
      "source": [
        "## Device retention (gradual) simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwHbxLqmEGFa",
        "scrolled": true
      },
      "source": [
        "import enum\n",
        "from enum import Enum, auto\n",
        "import math\n",
        "import pandas as pd\n",
        "from memtorch.mn.Module import supported_module_parameters\n",
        "import math\n",
        "from memtorch.mn.Module import patch_model\n",
        "from memtorch.map.Module import naive_tune\n",
        "from memtorch.map.Parameter import naive_map\n",
        "from memtorch.bh.crossbar.Program import naive_program\n",
        "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
        "from memtorch.bh.crossbar.Crossbar import init_crossbar\n",
        "import copy\n",
        "from mobilenetv2 import MobileNetV2\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "class OperationMode(Enum):\n",
        "    sudden = auto()\n",
        "    gradual = auto()\n",
        "\n",
        "    \n",
        "def minimal_tune(model):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if hasattr(m, 'tune'):\n",
        "            m.transform_output = lambda input: input\n",
        "            if isinstance(m, memtorch.mn.Conv2d):\n",
        "                m.transform_output = naive_tune(m, (4, m.in_channels, 8, 8))\n",
        "            if isinstance(m, memtorch.mn.Linear):\n",
        "                m.transform_output = naive_tune(m, (64, m.in_features))\n",
        "                \n",
        "    return model.to(device)\n",
        "    \n",
        "def gradual(input, x, p_0, p_1, p_2, p_3, cell_size, tempurature):\n",
        "    p_0 = p_0 / input\n",
        "    if tempurature > 298:\n",
        "        input = torch.pow(10, p_0 * tempurature + torch.log10(input) - p_0 * 298)\n",
        "        \n",
        "    p_0 = torch.log10(input)\n",
        "    threshold = p_1 * np.exp(p_2 * cell_size)\n",
        "    return torch.pow(10, (p_3 * cell_size * math.log10(x) + torch.log10(10 **  p_0) - p_3 * cell_size * math.log10(threshold)))\n",
        "    \n",
        "def model_gradual(layer, x, tempurature):\n",
        "    cell_size = 10\n",
        "    convergence_point = 180000\n",
        "    p_0_lrs = 0.000801158151673717 * 2400\n",
        "    p_0_hrs = 0.00420717061486765 * 55000\n",
        "    p_1 = 0.5\n",
        "    p_2 = 0.7600902459542083\n",
        "    p_3_lrs = 0.006489105105825544  \n",
        "    p_3_hrs = -0.007240917429683966\n",
        "    threshold_lrs = p_1 * math.exp(p_2 * cell_size)\n",
        "    threshold_hrs = p_1 * math.exp(p_2 * cell_size)\n",
        "    for i in range(len(layer.crossbars)):\n",
        "        input = 1 / layer.crossbars[i].conductance_matrix\n",
        "        if input[input < convergence_point].nelement() > 0:\n",
        "            if x > threshold_lrs:\n",
        "                input[input < convergence_point] = gradual(input[input < convergence_point], time_, p_0_lrs, p_1, p_2, p_3_lrs, cell_size, tempurature)\n",
        "        if input[input > convergence_point].nelement() > 0:\n",
        "            if x > threshold_hrs:\n",
        "                input[input > convergence_point] = gradual(input[input > convergence_point], time_, p_0_hrs, p_1, p_2, p_3_hrs, cell_size, tempurature)\n",
        "\n",
        "        layer.crossbars[i].conductance_matrix = 1 / input\n",
        "\n",
        "    return layer\n",
        "    \n",
        "def model_degradation(model, time_, operation_mode, tempurature):\n",
        "    for i, (name, m) in enumerate(list(model.named_modules())):\n",
        "        if type(m) in supported_module_parameters.values():\n",
        "            if len(name.split('.')) > 1:\n",
        "                name = name.split('.')[1]\n",
        "\n",
        "            if operation_mode == OperationMode.gradual:\n",
        "                if hasattr(model, 'module'):\n",
        "                    setattr(model.module, name, model_gradual(m, time_, tempurature))\n",
        "                else:\n",
        "                    setattr(model, name, model_gradual(m, time_, tempurature))\n",
        "            elif operation_mode == OperationMode.sudden:\n",
        "                if hasattr(model, 'module'):\n",
        "                    setattr(model.module, name, model_sudden(m, time_, tempurature))\n",
        "                else:\n",
        "                    setattr(model, name, model_sudden(m, time_, tempurature))\n",
        "                    \n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=1)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
        "reference_memristor = memtorch.bh.memristor.VTEAM\n",
        "r_on = 9e4\n",
        "r_off = 330000\n",
        "reference_memristor_params = {'time_series_resolution': 1e-10,\n",
        "                              'r_off': r_off,\n",
        "                              'r_on': r_on}\n",
        "times = 10 ** np.arange(1, 10, dtype=np.float64)\n",
        "tempuratures = np.linspace(75, 175, 10, endpoint=True)\n",
        "df = pd.DataFrame(columns=['time', 'tempurature', 'test_set_accuracy'])\n",
        "for time_ in times:\n",
        "    print(time_)\n",
        "    for tempurature in tempuratures:\n",
        "        tempurature += 273\n",
        "        model = MobileNetV2().to(device)\n",
        "        model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
        "        patched_model = patch_model(model,\n",
        "                                  memristor_model=reference_memristor,\n",
        "                                  memristor_model_params=reference_memristor_params,\n",
        "                                  module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
        "                                  mapping_routine=naive_map,\n",
        "                                  transistor=True,\n",
        "                                  programming_routine=None,\n",
        "                                  p_l=None,\n",
        "                                  scheme=memtorch.bh.Scheme.DoubleColumn)\n",
        "\n",
        "        patched_model = minimal_tune(patched_model)\n",
        "        patched_model = model_degradation(patched_model, time_, OperationMode.gradual, tempurature)\n",
        "        accuracy = test(patched_model, test_loader)\n",
        "        del patched_model\n",
        "        df = df.append({'time': time_, 'tempurature': tempurature, 'test_set_accuracy': accuracy}, ignore_index=True)\n",
        "        df.to_csv('retention_gradual.csv', index=False)\n",
        "        if SAVE_GOOGLE_COLAB:\n",
        "          shutil.copy(\"retention_gradual.csv\", \"/content/gdrive/My Drive/retention_gradual.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}